{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task VI (b)\n",
        " Take the pre-trained model from Task VI.A and fine-tune it for a super-resolution task. The model should be fine-tuned to upscale low-resolution strong lensing images using the provided high-resolution samples as ground truths. Please implement your approach in PyTorch or Keras and discuss your strategy.\n"
      ],
      "metadata": {
        "id": "l5DrQ7gpQM_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breakdown to approach:\n",
        "* Using Smart Upsampling\n",
        "\n",
        "* Transpose Convolutions & PixelShuffle help scale up the image while preserving fine details.\n",
        "\n",
        "* A Tanh activation at the end smooths the output, making high-resolution (HR) images look more natural.\n",
        "\n",
        "* Extracting Meaningful Features from the Pre-trained Model\n",
        "\n",
        "* The MAE model has already learned useful patterns from lensing images.\n",
        "\n",
        "* Instead of retraining everything, we freeze early layers and fine-tune the important ones to adapt to super-resolution.\n",
        "\n",
        "* Better Loss Function & Optimization for Sharper Images\n",
        "\n",
        "* L1 Loss instead of MSE: Helps keep fine details sharp instead of making images blurry.\n",
        "\n",
        "* AdamW Optimizer: Helps stabilize training and avoids overfitting, which is useful for scientific images.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "be7xiu26QV5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "oMXndZ2aQb7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model loading and prep"
      ],
      "metadata": {
        "id": "6GYV6yQkQnKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained MAE classifier model\n",
        "model_path = \"mae_classifier.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# Remove 'mae.' prefix if present\n",
        "new_state_dict = {k.replace(\"mae.\", \"\"): v for k, v in state_dict.items()}\n",
        "\n",
        "# Remove classifier layer keys from state_dict\n",
        "new_state_dict = {k: v for k, v in new_state_dict.items() if not k.startswith(\"fc.\")}\n",
        "\n",
        "pretrained_model = resnet18(pretrained=False)\n",
        "pretrained_model.load_state_dict(new_state_dict, strict=False)\n"
      ],
      "metadata": {
        "id": "bu-g1jbOQrrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Modify the model for super-resolution using MAE\n",
        "class SuperResolutionMAE(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(SuperResolutionMAE, self).__init__()\n",
        "\n",
        "        # Extract feature encoder from MAE (usually the transformer backbone)\n",
        "        self.feature_extractor = base_model.encoder  # Adjust based on your MAE model structure\n",
        "\n",
        "        # Upsampling module for super-resolution\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Conv2d(768, 512, kernel_size=3, stride=1, padding=1),  # Adjust input channels based on MAE\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)  # Extract features using MAE encoder\n",
        "        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False)  # Upsampling\n",
        "        x = self.upsample(x)  # Refinement with CNN layers\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = SuperResolutionMAE(pretrained_model)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "I-eL2lZK9Z5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "hqTEG2nnRBlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load pre-trained MAE classifier model\n",
        "model_path = \"mae_classifier.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# Remove 'mae.' prefix if present\n",
        "new_state_dict = {k.replace(\"mae.\", \"\"): v for k, v in state_dict.items()}\n",
        "\n",
        "# Remove classifier layer keys if they exist\n",
        "new_state_dict = {k: v for k, v in new_state_dict.items() if not k.startswith(\"classifier.\")}\n",
        "\n",
        "# Load modified MAE model\n",
        "from mae_model import MaskedAutoencoder  # Import your MAE model class\n",
        "\n",
        "pretrained_model = MaskedAutoencoder()  # Initialize your MAE model\n",
        "pretrained_model.load_state_dict(new_state_dict, strict=False)\n"
      ],
      "metadata": {
        "id": "SA1vafB9RJS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Super-Resolution Model using Transformer-based MAE Features\n",
        "class SuperResolutionMAE(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(SuperResolutionMAE, self).__init__()\n",
        "\n",
        "        # Extract the MAE encoder (transformer backbone)\n",
        "        self.feature_extractor = base_model.encoder  # Adjust based on your MAE model structure\n",
        "\n",
        "        # Upsampling module using Transpose Convolutions and PixelShuffle\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Conv2d(768, 512, kernel_size=3, stride=1, padding=1),  # Adjust input channels based on MAE\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 4, kernel_size=3, stride=1, padding=1),  # 4 channels for PixelShuffle\n",
        "            nn.PixelShuffle(2),  # Upscales by 2x\n",
        "            nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1),  # Final refinement layer\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)  # Extract features using MAE encoder\n",
        "        x = x.permute(0, 2, 1).view(x.shape[0], 768, 14, 14)  # Reshape to (B, C, H, W) if needed\n",
        "        x = self.upsample(x)  # Apply upsampling\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mvoY9ioURNrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Initialize MAE-based Super-Resolution Model\n",
        "model = SuperResolutionMAE(pretrained_model)\n",
        "\n",
        "# Freeze early layers (Transformer Encoder)\n",
        "for param in list(model.feature_extractor.parameters())[:len(list(model.feature_extractor.parameters())) // 2]:\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define L1 Loss and AdamW optimizer\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Custom Dataset Class\n",
        "class SuperResolutionDataset(Dataset):\n",
        "    def __init__(self, dataset_path):\n",
        "        self.lr_path = os.path.join(dataset_path, \"LR\")\n",
        "        self.hr_path = os.path.join(dataset_path, \"HR\")\n",
        "        self.lr_files = sorted(os.listdir(self.lr_path))\n",
        "        self.hr_files = sorted(os.listdir(self.hr_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lr_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lr_image = np.load(os.path.join(self.lr_path, self.lr_files[idx])).astype(np.float32)\n",
        "        hr_image = np.load(os.path.join(self.hr_path, self.hr_files[idx])).astype(np.float32)\n",
        "\n",
        "        # Ensure correct dimensions\n",
        "        if len(lr_image.shape) == 2: lr_image = np.expand_dims(lr_image, axis=0)  # Add channel dim\n",
        "        if len(hr_image.shape) == 2: hr_image = np.expand_dims(hr_image, axis=0)\n",
        "\n",
        "        lr_image = torch.tensor(lr_image, dtype=torch.float32)\n",
        "        hr_image = torch.tensor(hr_image, dtype=torch.float32)\n",
        "\n",
        "        return lr_image, hr_image\n",
        "\n",
        "# Load Dataset\n",
        "dataset_path = \"Dataset2/Dataset\"\n",
        "dataset = SuperResolutionDataset(dataset_path)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for lr_images, hr_images in train_loader:\n",
        "            lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(lr_images)\n",
        "\n",
        "            # Ensure output size matches target size\n",
        "            outputs = F.interpolate(outputs, size=hr_images.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            loss = criterion(outputs, hr_images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.6f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48SpUBqP2xj_",
        "outputId": "23121612-5895-4dc4-d61d-e3c762a2057c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.040184\n",
            "Epoch [2/20], Loss: 0.025419\n",
            "Epoch [3/20], Loss: 0.022257\n",
            "Epoch [4/20], Loss: 0.019837\n",
            "Epoch [5/20], Loss: 0.018492\n",
            "Epoch [6/20], Loss: 0.018193\n",
            "Epoch [7/20], Loss: 0.017289\n",
            "Epoch [8/20], Loss: 0.016779\n",
            "Epoch [9/20], Loss: 0.016173\n",
            "Epoch [10/20], Loss: 0.016080\n",
            "Epoch [11/20], Loss: 0.015976\n",
            "Epoch [12/20], Loss: 0.015278\n",
            "Epoch [13/20], Loss: 0.015062\n",
            "Epoch [14/20], Loss: 0.014190\n",
            "Epoch [15/20], Loss: 0.013255\n",
            "Epoch [16/20], Loss: 0.012234\n",
            "Epoch [17/20], Loss: 0.011827\n",
            "Epoch [18/20], Loss: 0.011456\n",
            "Epoch [19/20], Loss: 0.011448\n",
            "Epoch [20/20], Loss: 0.011247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    avg_psnr, avg_ssim, avg_mse = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for lr_images, hr_images in test_loader:\n",
        "            lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
        "            outputs = model(lr_images)\n",
        "            outputs = F.interpolate(outputs, size=hr_images.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            outputs = outputs.cpu().numpy()\n",
        "            hr_images = hr_images.cpu().numpy()\n",
        "\n",
        "            for i in range(len(outputs)):\n",
        "                avg_psnr += psnr(hr_images[i, 0], outputs[i, 0], data_range=1.0)\n",
        "                avg_ssim += ssim(hr_images[i, 0], outputs[i, 0], data_range=1.0)\n",
        "                avg_mse += np.mean((hr_images[i, 0] - outputs[i, 0]) ** 2)\n",
        "\n",
        "    avg_psnr /= len(test_loader.dataset)\n",
        "    avg_ssim /= len(test_loader.dataset)\n",
        "    avg_mse /= len(test_loader.dataset)\n",
        "    print(f\"Average PSNR: {avg_psnr:.2f} dB, Average SSIM: {avg_ssim:.4f}, Average MSE: {avg_mse:.6f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPryo3dyWK0l",
        "outputId": "c822eef8-cf45-4880-e4c7-5b85420f6654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average PSNR: 31.47 dB, Average SSIM: 0.9310, Average MSE: 0.000736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QV_7T1AZUrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}