{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11276562,"sourceType":"datasetVersion","datasetId":7049791}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task III (b)\nTrain a deep learning-based super-resolution algorithm of your choice to enhance low-resolution strong lensing images using a limited dataset of real HR/LR pairs collected from HSC and HST telescopes. You can adapt and fine-tune your super-resolution model from Task III.A. or use any other approach, such as few-shot learning strategies, transfer learning, domain adaptation, or data augmentation techniques, etc. Please implement your approach in PyTorch or Keras and discuss your strategy.\n","metadata":{}},{"cell_type":"markdown","source":"**Breakdown to approach**\nOur approach involves fine-tuning an Enhanced Deep Super-Resolution (EDSR) model to improve the resolution of strong lensing images. The entire pipeline includes data handling, model architecture, training, and evaluation.","metadata":{}},{"cell_type":"markdown","source":"* Residual Blocks: 16 blocks with skip connections to enhance feature extraction.\n\n* Upsampling Layer: Using PixelShuffle to upscale images better.\n\n* Final Convolution: Converts the upsampled output into a single-channel grayscale image.","metadata":{}},{"cell_type":"markdown","source":"**Loss Function:** L1 loss, as it helps preserve structural details better than L2 loss.\n\n**Optimizer:** Adam optimizer with a learning rate of 1e-4.\n\n**Training Pipeline:**\n\n* Loads LR and HR images as mini-batches.\n\n* Feeds LR images into EDSR to generate SR images.\n\n* Compares SR images with HR ground truth using L1 loss.\n\n* Updates model weights using backpropagation.","metadata":{}},{"cell_type":"markdown","source":"## ESDR Model\nThe EDSR architecture is based on the SRResNet architecture and consists of multiple residual blocks. It uses constant scaling layers instead of batch normalization layers to produce consistent results (input and output have similar distributions, thus normalizing intermediate features may not be desirable). Instead of using a L2 loss (mean squared error), the authors employed an L1 loss (mean absolute error), which performs better empirically. Read more [here](https://keras.io/examples/vision/edsr/) and [here](https://medium.com/axinc-ai/edsr-a-machine-learning-model-for-super-resolution-image-processing-9deaf36b24ed)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os\n\n\n# A custom PyTorch Dataset class loads these .npy files, applies transformations (converts them to tensors), and prepares them for training.\n\n# Dataset class for handling HR and LR .npy files\nclass SuperResolutionDataset(Dataset):\n    def __init__(self, lr_dir, hr_dir, transform=None):\n        self.lr_files = sorted(os.listdir(lr_dir))\n        self.hr_files = sorted(os.listdir(hr_dir))\n        self.lr_dir = lr_dir\n        self.hr_dir = hr_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.lr_files)\n    \n    def __getitem__(self, idx):\n        lr_path = os.path.join(self.lr_dir, self.lr_files[idx])\n        hr_path = os.path.join(self.hr_dir, self.hr_files[idx])\n        \n        lr_image = np.load(lr_path).astype(np.float32)\n        hr_image = np.load(hr_path).astype(np.float32)\n        \n        # Convert (1, H, W) to (H, W) for transformation\n        lr_image = lr_image.squeeze()\n        hr_image = hr_image.squeeze()\n        \n        if self.transform:\n            lr_image = self.transform(lr_image)\n            hr_image = self.transform(hr_image)\n        \n        return lr_image, hr_image\n\n# Image transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert to tensor\n])\n\n# Define EDSR Model\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n    \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        return out + x  # Skip connection\n\n#  EDSR removes batch normalization layers to reduce memory consumption and improve performance over traditional SR methods like SRCNN.\n\nclass EDSR(nn.Module):\n    def __init__(self, num_blocks=16, channels=64, scale_factor=2):\n        super(EDSR, self).__init__()\n        self.conv1 = nn.Conv2d(1, channels, kernel_size=3, padding=1)\n        self.res_blocks = nn.Sequential(*[ResidualBlock(channels) for _ in range(num_blocks)])\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(channels, channels * scale_factor**2, kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor),\n            nn.Conv2d(channels, 1, kernel_size=3, padding=1)\n        )\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.res_blocks(x)\n        x = self.conv2(x)\n        x = self.upsample(x)\n        return x\n\n# Training parameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = EDSR().to(device)\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\ndef train(model, dataloader, criterion, optimizer, epochs=50):\n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for lr_imgs, hr_imgs in dataloader:\n            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n            optimizer.zero_grad()\n            sr_imgs = model(lr_imgs)\n            loss = criterion(sr_imgs, hr_imgs)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")\n\n# Load dataset\nlr_dir = \"/kaggle/input/dataset3b/Dataset/LR\"\nhr_dir = \"/kaggle/input/dataset3b/Dataset/HR\"\ndataset = SuperResolutionDataset(lr_dir, hr_dir, transform=transform)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n# Model Training\ntrain(model, dataloader, criterion, optimizer, epochs=50)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T15:35:12.374338Z","iopub.execute_input":"2025-04-04T15:35:12.374699Z","iopub.status.idle":"2025-04-04T16:57:09.261740Z","shell.execute_reply.started":"2025-04-04T15:35:12.374668Z","shell.execute_reply":"2025-04-04T16:57:09.260506Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/50], Loss: 0.0251\nEpoch [2/50], Loss: 0.0174\nEpoch [3/50], Loss: 0.0170\nEpoch [4/50], Loss: 0.0171\nEpoch [5/50], Loss: 0.0159\nEpoch [6/50], Loss: 0.0160\nEpoch [7/50], Loss: 0.0167\nEpoch [8/50], Loss: 0.0161\nEpoch [9/50], Loss: 0.0174\nEpoch [10/50], Loss: 0.0162\nEpoch [11/50], Loss: 0.0159\nEpoch [12/50], Loss: 0.0160\nEpoch [13/50], Loss: 0.0163\nEpoch [14/50], Loss: 0.0156\nEpoch [15/50], Loss: 0.0155\nEpoch [16/50], Loss: 0.0166\nEpoch [17/50], Loss: 0.0158\nEpoch [18/50], Loss: 0.0161\nEpoch [19/50], Loss: 0.0162\nEpoch [20/50], Loss: 0.0165\nEpoch [21/50], Loss: 0.0165\nEpoch [22/50], Loss: 0.0155\nEpoch [23/50], Loss: 0.0154\nEpoch [24/50], Loss: 0.0155\nEpoch [25/50], Loss: 0.0154\nEpoch [26/50], Loss: 0.0161\nEpoch [27/50], Loss: 0.0158\nEpoch [28/50], Loss: 0.0157\nEpoch [29/50], Loss: 0.0155\nEpoch [30/50], Loss: 0.0157\nEpoch [31/50], Loss: 0.0157\nEpoch [32/50], Loss: 0.0155\nEpoch [33/50], Loss: 0.0154\nEpoch [34/50], Loss: 0.0152\nEpoch [35/50], Loss: 0.0155\nEpoch [36/50], Loss: 0.0152\nEpoch [37/50], Loss: 0.0153\nEpoch [38/50], Loss: 0.0158\nEpoch [39/50], Loss: 0.0155\nEpoch [40/50], Loss: 0.0156\nEpoch [41/50], Loss: 0.0153\nEpoch [42/50], Loss: 0.0155\nEpoch [43/50], Loss: 0.0153\nEpoch [44/50], Loss: 0.0156\nEpoch [45/50], Loss: 0.0155\nEpoch [46/50], Loss: 0.0154\nEpoch [47/50], Loss: 0.0154\nEpoch [48/50], Loss: 0.0154\nEpoch [49/50], Loss: 0.0153\nEpoch [50/50], Loss: 0.0156\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport math\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\n\n# Updated Evaluation function\ndef evaluate(model, dataloader):\n    model.eval()\n    total_psnr = 0\n    total_ssim = 0\n    count = 0\n    with torch.no_grad():\n        for lr_imgs, hr_imgs in dataloader:\n            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n            sr_imgs = model(lr_imgs)\n\n            for i in range(sr_imgs.size(0)):\n                sr_img = sr_imgs[i].squeeze().cpu().numpy()\n                hr_img = hr_imgs[i].squeeze().cpu().numpy()\n\n                # Ensure the image is large enough for SSIM window or reduce window size\n                min_dim = min(sr_img.shape)\n                window_size = min(7, min_dim if min_dim % 2 == 1 else min_dim - 1)\n\n                psnr_value = psnr(hr_img, sr_img, data_range=1.0)\n                ssim_value = ssim(hr_img, sr_img, data_range=1.0, win_size=window_size)\n\n                total_psnr += psnr_value\n                total_ssim += ssim_value\n                count += 1\n\n    avg_psnr = total_psnr / count\n    avg_ssim = total_ssim / count\n    print(f\"Average PSNR: {avg_psnr:.4f}, Average SSIM: {avg_ssim:.4f}\")\n    return avg_psnr, avg_ssim\n\n# Evaluate the model\nevaluate(model, dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:00:40.322134Z","iopub.execute_input":"2025-04-04T17:00:40.322510Z","iopub.status.idle":"2025-04-04T17:01:13.114336Z","shell.execute_reply.started":"2025-04-04T17:00:40.322481Z","shell.execute_reply":"2025-04-04T17:01:13.112815Z"}},"outputs":[{"name":"stdout","text":"Average PSNR: 34.8793, Average SSIM: 0.8526\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(34.87930544052202, 0.8526361402240176)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"PSNR (Peak Signal-to-Noise Ratio): Measures how much the generated SR image resembles the HR image.\n\nSSIM (Structural Similarity Index): Evaluates perceptual similarity.","metadata":{}},{"cell_type":"markdown","source":"**Scope of Improvement:**\n* Data Augmentation: Rotation, flipping, and noise addition can improve generalization.\n* Perceptual Loss: Combining L1 loss with VGG-based perceptual loss could yield sharper results.\n* can try GAN-Based Approaches: ESRGAN or SRGAN can enhance textures.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}