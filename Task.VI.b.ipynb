{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task VI (b)\n",
        " Take the pre-trained model from Task VI.A and fine-tune it for a super-resolution task. The model should be fine-tuned to upscale low-resolution strong lensing images using the provided high-resolution samples as ground truths. Please implement your approach in PyTorch or Keras and discuss your strategy.\n"
      ],
      "metadata": {
        "id": "l5DrQ7gpQM_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breakdown to approach:\n",
        "* Our pre-trained model has already learned representations of lensing images from TaskVI (a). Instead of learning from scratch, this method refines learned knowledge and adapts it to a different but related task  (super-resolution).\n",
        "* Upsampling (Transpose Convolutions + PixelShuffle): Enhances resolution while maintaining structural integrity. Final Refinement (Tanh Activation): Ensures smoother HR reconstructions.\n",
        "* Domain specific Feature Extraction.\n",
        "* L1 Loss: Reduces blurriness compared to MSE, improving fine detail reconstruction. AdamW Optimizer: Stabilizes training and prevents overfitting with weight decay.\n",
        "\n"
      ],
      "metadata": {
        "id": "be7xiu26QV5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "oMXndZ2aQb7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model loading and prep"
      ],
      "metadata": {
        "id": "6GYV6yQkQnKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ResNet classifier model\n",
        "model_path = \"resnet_classifier.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# Remove 'resnet.' prefix if present\n",
        "new_state_dict = {k.replace(\"resnet.\", \"\"): v for k, v in state_dict.items()}\n",
        "\n",
        "# Remove classifier layer keys from state_dict\n",
        "new_state_dict = {k: v for k, v in new_state_dict.items() if not k.startswith(\"fc.\")}\n",
        "\n",
        "pretrained_model = resnet18(pretrained=False)\n",
        "pretrained_model.load_state_dict(new_state_dict, strict=False)\n"
      ],
      "metadata": {
        "id": "bu-g1jbOQrrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the model for super-resolution\n",
        "class SuperResolutionResNet(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(SuperResolutionResNet, self).__init__()\n",
        "\n",
        "        # Modify input layer to accept 1-channel grayscale images\n",
        "        base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Extract convolutional layers only (remove fully connected layer)\n",
        "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-2])\n",
        "\n",
        "        # Upsampling module for super-resolution\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False)\n",
        "        x = self.upsample(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SuperResolutionResNet(pretrained_model)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Lets understand Model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrehsEcIYLAA",
        "outputId": "93ad8bf1-a4a1-46e6-c3e8-0bb5d571c0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuperResolutionResNet(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (upsample): Sequential(\n",
              "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "hqTEG2nnRBlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained ResNet classifier model\n",
        "model_path = \"resnet_classifier.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# Remove 'resnet.' prefix\n",
        "new_state_dict = {k.replace(\"resnet.\", \"\"): v for k, v in state_dict.items()}\n",
        "# Remove classifier layer keys\n",
        "new_state_dict = {k: v for k, v in new_state_dict.items() if not k.startswith(\"fc.\")}\n",
        "\n",
        "# Load modified ResNet\n",
        "pretrained_model = resnet18(pretrained=False)\n",
        "pretrained_model.load_state_dict(new_state_dict, strict=False)\n"
      ],
      "metadata": {
        "id": "SA1vafB9RJS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Super-Resolution Model using Transpose Convolutions and PixelShuffle\n",
        "class SuperResolutionResNet(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(SuperResolutionResNet, self).__init__()\n",
        "\n",
        "        # Modify input layer to accept grayscale images\n",
        "        base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Extract convolutional layers\n",
        "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-2])\n",
        "\n",
        "        # Upsampling module\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 4, kernel_size=3, stride=1, padding=1),  # 4 channels for pixel shuffle\n",
        "            nn.PixelShuffle(2),  # Upscales by 2x\n",
        "            nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1),  # Final refinement layer\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.upsample(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mvoY9ioURNrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = SuperResolutionResNet(pretrained_model)\n",
        "\n",
        "# Freeze early layers and fine-tune only later layers\n",
        "for param in model.feature_extractor[:6].parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define L1 Loss and AdamW optimizer\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Custom Dataset Class\n",
        "class SuperResolutionDataset(Dataset):\n",
        "    def __init__(self, dataset_path):\n",
        "        self.lr_path = os.path.join(dataset_path, \"LR\")\n",
        "        self.hr_path = os.path.join(dataset_path, \"HR\")\n",
        "        self.lr_files = sorted(os.listdir(self.lr_path))\n",
        "        self.hr_files = sorted(os.listdir(self.hr_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lr_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lr_image = np.load(os.path.join(self.lr_path, self.lr_files[idx]))\n",
        "        hr_image = np.load(os.path.join(self.hr_path, self.hr_files[idx]))\n",
        "\n",
        "        # Ensure correct dimensions\n",
        "        lr_image = np.expand_dims(lr_image, axis=0) if len(lr_image.shape) == 2 else lr_image\n",
        "        hr_image = np.expand_dims(hr_image, axis=0) if len(hr_image.shape) == 2 else hr_image\n",
        "\n",
        "        lr_image = torch.tensor(lr_image, dtype=torch.float32)\n",
        "        hr_image = torch.tensor(hr_image, dtype=torch.float32)\n",
        "\n",
        "        return lr_image, hr_image\n",
        "\n",
        "# Load Dataset\n",
        "dataset_path = \"Dataset2/Dataset\"\n",
        "dataset = SuperResolutionDataset(dataset_path)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for lr_images, hr_images in train_loader:\n",
        "            lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(lr_images)\n",
        "\n",
        "            # Ensure output size matches target size\n",
        "            outputs = F.interpolate(outputs, size=hr_images.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            loss = criterion(outputs, hr_images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.6f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48SpUBqP2xj_",
        "outputId": "23121612-5895-4dc4-d61d-e3c762a2057c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.040184\n",
            "Epoch [2/20], Loss: 0.025419\n",
            "Epoch [3/20], Loss: 0.022257\n",
            "Epoch [4/20], Loss: 0.019837\n",
            "Epoch [5/20], Loss: 0.018492\n",
            "Epoch [6/20], Loss: 0.018193\n",
            "Epoch [7/20], Loss: 0.017289\n",
            "Epoch [8/20], Loss: 0.016779\n",
            "Epoch [9/20], Loss: 0.016173\n",
            "Epoch [10/20], Loss: 0.016080\n",
            "Epoch [11/20], Loss: 0.015976\n",
            "Epoch [12/20], Loss: 0.015278\n",
            "Epoch [13/20], Loss: 0.015062\n",
            "Epoch [14/20], Loss: 0.014190\n",
            "Epoch [15/20], Loss: 0.013255\n",
            "Epoch [16/20], Loss: 0.012234\n",
            "Epoch [17/20], Loss: 0.011827\n",
            "Epoch [18/20], Loss: 0.011456\n",
            "Epoch [19/20], Loss: 0.011448\n",
            "Epoch [20/20], Loss: 0.011247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    avg_psnr, avg_ssim, avg_mse = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for lr_images, hr_images in test_loader:\n",
        "            lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
        "            outputs = model(lr_images)\n",
        "            outputs = F.interpolate(outputs, size=hr_images.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            outputs = outputs.cpu().numpy()\n",
        "            hr_images = hr_images.cpu().numpy()\n",
        "\n",
        "            for i in range(len(outputs)):\n",
        "                avg_psnr += psnr(hr_images[i, 0], outputs[i, 0], data_range=1.0)\n",
        "                avg_ssim += ssim(hr_images[i, 0], outputs[i, 0], data_range=1.0)\n",
        "                avg_mse += np.mean((hr_images[i, 0] - outputs[i, 0]) ** 2)\n",
        "\n",
        "    avg_psnr /= len(test_loader.dataset)\n",
        "    avg_ssim /= len(test_loader.dataset)\n",
        "    avg_mse /= len(test_loader.dataset)\n",
        "    print(f\"Average PSNR: {avg_psnr:.2f} dB, Average SSIM: {avg_ssim:.4f}, Average MSE: {avg_mse:.6f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPryo3dyWK0l",
        "outputId": "c822eef8-cf45-4880-e4c7-5b85420f6654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average PSNR: 31.47 dB, Average SSIM: 0.9310, Average MSE: 0.000736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QV_7T1AZUrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
